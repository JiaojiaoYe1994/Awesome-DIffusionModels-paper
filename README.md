# Awesome-DIffusionModels4Multi-Modal
A curated list of Diffusion Models for Mutli-Modal Generation with awesome resources (paper, code, application, review, survey, etc.), which aims to comprehensively and systematically summarize the recent advances to the best of our knowledge.

## Table of contents
### 1. Text-to-Image Generation
### 2. Scene Graph-to-Image Generation
### 3. Text-to-3D Generation
### 4. Text-to-Motion Generation
### 5. Text-to-Video Generation
### 6. Text-to-Audio Generation

## 1. Text-to-Image Generation
* A survey of vision-language pre-trained models. (2022). Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. 2022. [paper](https://arxiv.org/abs/2202.10936)
* Imagic: Text-Based Real Image Editing with Diffusion Models. (2022) Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2022. [paper](https://arxiv.org/abs/2210.09276)
* Least squares estimation without priors or supervision. Neural computation 23, 2 (2011), 374–420 Martin Raphan and Eero P Simoncelli. 2011.  [paper](https://dl.acm.org/doi/10.1162/NECO_a_00076)
* UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image. SIGGRAPH 2023 (2022) Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. 2022. [paper](https://arxiv.org/abs/2210.09477) (2022). 
* Hierarchical text-conditional image generation with clip latents. NiPs (2022). Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. [paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf)
* GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. ICML. 16784–16804. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022.  [paper](https://arxiv.org/abs/2112.10741)
* Vector quantized diffusion model for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition. 10696–10706. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. 2022. [paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf)

## 2. Scene Graph-to-Image Generation

## 3. Text-to-3D Generation

## 4. Text-to-Motion Generation

## 5. Text-to-Video Generation

## 6. Text-to-Audio Generation
